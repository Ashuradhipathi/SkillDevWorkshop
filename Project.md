[FSGM Tutorial](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)
# algo for det-adv-prop :
![Image](https://github.com/Ashuradhipathi/SkillDevWorkshop/blob/main/Screenshot%202024-03-15%20112019.png)

More concretely, we use the FGSM algorithm [10] with
non-targeted attack to explain Det-AdvProp without loss of
generality. Given an input image x and a bounding box b
over an object of class y, FGSM produces the adversarial
examples by a one-step projected gradient descent:
Ë†xcls = P(x +  Â· sign(âˆ‡xLcls(x, y; Î¸))), (3)
Ë†xloc = P(x +  Â· sign(âˆ‡xLloc(x, b; Î¸))), (4)
Ë†x = arg max
xâˆˆ{Ë†xcls,Ë†xloc}
Ldet(x, y, b; Î¸), (5)
where  is the attack strength and P denotes the projection
onto the norm ball {Ë†x | kË†x âˆ’ xkâˆ â‰¤ }. We first generate
two adversarial examples by maximizing the single task loss
and then choose the one that maximizes the total loss of the
detection task. Equation (5) is the max-max rule to keep the
adversarial example (out of two) that maximizes the total
detection loss Ldet. The inner â€œmaxâ€ in the max-max rule
refers to the adversarial examples that maximize Lcls and
Lloc, respectively, and the outer â€œmaxâ€ indicates we take
the one that maximizes the total detection loss Ldet. The
overall training objective is given below:
min
Î¸
Exâˆ¼D;y,bâˆ¼B(x)Ldet(x, y, b; Î¸) + Ldet(Ë†x, y, b; Î¸). (6)
Intuitively, this max-max scheme lets the stronger one
survive between the two adversarial examples Ë†xcls and Ë†xloc.
By separately attacking the classification and localization
branches, we avoid the gradient misalignment problem.
Since we only produce one adversarial example per clean
image, coupled with one auxiliary batchnorm, the training
does not suffer from the excessive regularization problem
mentioned earlier


